#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MEGå»å™ªè®­ç»ƒè„šæœ¬
ä¸“é—¨å¤„ç†fTé‡çº§çš„MEGä¿¡å·
ä½¿ç”¨æˆå¯¹çš„å­é‡‡æ ·ä¿¡å·è¿›è¡Œè‡ªç›‘ç£è®­ç»ƒ
"""

import os
import gc
import torch
import warnings
import numpy as np
from pathlib import Path
from tqdm import tqdm
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
from datetime import datetime
import shutil

from MEG_dataset import MEGDataset, subsample2_meg
from MEG_model import SimpleMEGDenoiser, MEGDenoiser, AttentionMEGDenoiser
from MEG_loss import SimpleMEGLoss, MEGLoss, SelfSupervisedMEGLoss

# æ£€æŸ¥GPUå¯ç”¨æ€§
train_on_gpu = torch.cuda.is_available()
if train_on_gpu:
    print('âœ… åœ¨GPUä¸Šè®­ç»ƒ')
else:
    print('âš ï¸  åœ¨CPUä¸Šè®­ç»ƒï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰')
DEVICE = torch.device('cuda' if train_on_gpu else 'cpu')

warnings.filterwarnings(action='ignore', category=DeprecationWarning)
np.random.seed(999)
torch.manual_seed(999)

# CUDAç¡®å®šæ€§è®¾ç½®
if train_on_gpu:
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

############################################## è®­ç»ƒå‚æ•° ##############################################
BATCH_SIZE = 16  # å¯ä»¥ä½¿ç”¨æ›´å¤§çš„batch size
LEARNING_RATE = 1e-4
EPOCHS = 200  # è®­ç»ƒé€šå¸¸æ”¶æ•›æ›´å¿«
SAVE_EVERY = 10  # æ¯5ä¸ªepochä¿å­˜ä¸€æ¬¡æ¨¡å‹
SIGNAL_LENGTH = 1001  # MEGä¿¡å·é•¿åº¦

def prepare_data():
    """å‡†å¤‡è®­ç»ƒå’ŒéªŒè¯æ•°æ®"""
    print("=== Preparing Training Data ===")
    
    # è·å–æ•°æ®æ–‡ä»¶è·¯å¾„
    all_noisy_files = sorted(list(Path('./all_whq/whq_train/noisy').rglob('*.mat')))
    all_clean_files = sorted(list(Path('./all_whq/whq_train/clean').rglob('*.mat')))
    
    print(f"Found {len(all_noisy_files)} noisy files")
    print(f"Found {len(all_clean_files)} clean files")
    
    # ä½¿ç”¨æ‰€æœ‰è®­ç»ƒæ•°æ®
    train_noisy = all_noisy_files
    train_clean = all_clean_files
    
    # è·å–éªŒè¯æ•°æ®
    val_noisy_files = sorted(list(Path('./all_whq/whq_valid/noisy').rglob('*.mat')))
    val_clean_files = sorted(list(Path('./all_whq/whq_valid/clean').rglob('*.mat')))
    
    print(f"Found {len(val_noisy_files)} validation noisy files")
    print(f"Found {len(val_clean_files)} validation clean files")
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = MEGDataset(train_noisy, train_clean, max_length=SIGNAL_LENGTH)
    val_dataset = MEGDataset(val_noisy_files, val_clean_files, max_length=SIGNAL_LENGTH)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)
    
    print(f"Training batches: {len(train_loader)}")
    print(f"Validation batches: {len(val_loader)}")
    
    return train_loader, val_loader

def train_one_epoch(model, train_loader, criterion, optimizer, epoch):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0.0
    
    pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}")
    
    for batch_idx, batch in enumerate(pbar):
        try:
            # è§£åŒ…æ—¶åŸŸæ•°æ®
            x_noisy, g1_meg, x_clean, g2_meg, _ = batch
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            x_noisy = x_noisy.to(DEVICE)
            g1_meg = g1_meg.to(DEVICE)
            g2_meg = g2_meg.to(DEVICE)
            
            # å‰å‘ä¼ æ’­
            optimizer.zero_grad()
            
            # å¯¹ç¬¬ä¸€ä¸ªå­é‡‡æ ·ä¿¡å·è¿›è¡Œå»å™ª
            fg1_meg = model(g1_meg)
            
            # å¯¹å®Œæ•´å™ªå£°ä¿¡å·å»å™ªå¹¶è¿›è¡Œå­é‡‡æ ·
            fx_meg = model(x_noisy)
            g1fx_list, g2fx_list = [], []
            for i in range(fx_meg.shape[0]):
                g1fx_sample, g2fx_sample = subsample2_meg(fx_meg[i].detach().cpu())
                g1fx_list.append(g1fx_sample)
                g2fx_list.append(g2fx_sample)
            g1_fy = torch.stack(g1fx_list).to(DEVICE)
            g2_fy = torch.stack(g2fx_list).to(DEVICE)
            
            # è®¡ç®—è‡ªç›‘ç£æŸå¤±
            loss = criterion(g1_meg, fg1_meg, g2_meg, g1_fy, g2_fy)
            
            # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºNaN
            if torch.isnan(loss):
                print(f"Warning: NaN loss in batch {batch_idx}, skipping")
                continue
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            total_loss += loss.item()
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'Loss': f'{loss.item():.6f}',
                'Avg_Loss': f'{total_loss/(batch_idx+1):.6f}'
            })
            
            # å†…å­˜æ¸…ç†
            if batch_idx % 50 == 0:
                torch.cuda.empty_cache() if train_on_gpu else None
                
        except Exception as e:
            print(f"Failed to train batch {batch_idx}: {e}")
            continue
    
    return total_loss / len(train_loader) if len(train_loader) > 0 else float('inf')

def validate(model, val_loader, criterion):
    """éªŒè¯æ¨¡å‹"""
    model.eval()
    total_loss = 0.0
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(val_loader):
            try:
                x_noisy, g1_meg, x_clean, g2_meg, _ = batch
                
                x_noisy = x_noisy.to(DEVICE)
                g1_meg = g1_meg.to(DEVICE)
                g2_meg = g2_meg.to(DEVICE)
                
                # å‰å‘ä¼ æ’­
                fg1_meg = model(g1_meg)
                
                # æŸå¤±è®¡ç®—
                fx_meg = model(x_noisy)
                # å¤„ç†æ‰¹æ¬¡æ•°æ®çš„å­é‡‡æ · (åˆ†ç¦»æ¢¯åº¦)
                g1fx_list, g2fx_list = [], []
                for i in range(fx_meg.shape[0]):
                    g1fx_sample, g2fx_sample = subsample2_meg(fx_meg[i].detach().cpu())
                    g1fx_list.append(g1fx_sample)
                    g2fx_list.append(g2fx_sample)
                g1fx = torch.stack(g1fx_list).to(DEVICE)
                g2fx = torch.stack(g2fx_list).to(DEVICE)
                
                loss = criterion(g1_meg, fg1_meg, g2_meg, g1fx, g2fx)
                
                if not torch.isnan(loss):
                    total_loss += loss.item()
                
            except Exception as e:
                print(f"Validation batch {batch_idx} failed: {e}")
                continue
    
    return total_loss / len(val_loader) if len(val_loader) > 0 else float('inf')

def visualize_denoising_results(model, val_loader, save_dir):
    """å¯è§†åŒ–å»å™ªæ•ˆæœ"""
    print("\n=== Visualizing Denoising Results ===")
    model.eval()
    
    # è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®
    for batch in val_loader:
        x_noisy, _, x_clean, _, _ = batch
        break
    
    # é€‰æ‹©ä¸€ä¸ªæ ·æœ¬è¿›è¡Œå¯è§†åŒ–
    sample_idx = 0
    noisy_signal = x_noisy[sample_idx].cpu().numpy()
    clean_signal = x_clean[sample_idx].cpu().numpy()
    
    # æ¨¡å‹å»å™ª
    with torch.no_grad():
        denoised_tensor = model(x_noisy.to(DEVICE))
    
    denoised_signal = denoised_tensor[sample_idx].cpu().numpy()
    
    # ç»˜å›¾
    plt.figure(figsize=(12, 8))
    
    # æ—¶åŸŸå¯è§†åŒ–
    plt.subplot(3, 1, 1)
    plt.plot(clean_signal[0])
    plt.title('Original Clean MEG Signal')
    plt.ylabel('Amplitude')
    plt.grid(True)
    
    plt.subplot(3, 1, 2)
    plt.plot(noisy_signal[0])
    plt.title('Noisy MEG Signal')
    plt.ylabel('Amplitude')
    plt.grid(True)
    
    plt.subplot(3, 1, 3)
    plt.plot(denoised_signal[0])
    plt.title('Denoised MEG Signal')
    plt.xlabel('Time Points')
    plt.ylabel('Amplitude')
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig(save_dir / 'time_domain_denoising_result.png')
    print(f"âœ… Time domain denoising result saved to {save_dir / 'time_domain_denoising_result.png'}")
    
    # è®¡ç®—ä¿¡å™ªæ¯”æ”¹å–„
    def calculate_snr(clean, noisy):
        noise = noisy - clean
        signal_power = np.mean(clean ** 2)
        noise_power = np.mean(noise ** 2)
        return 10 * np.log10(signal_power / noise_power)
    
    original_snr = calculate_snr(clean_signal, noisy_signal)
    denoised_snr = calculate_snr(clean_signal, denoised_signal)
    
    snr_improvement = denoised_snr - original_snr
    
    print(f"Original SNR: {original_snr:.2f} dB")
    print(f"Denoised SNR: {denoised_snr:.2f} dB")
    print(f"SNR Improvement: {snr_improvement:.2f} dB")
    
    return denoised_signal

def plot_training_history(train_losses, val_losses, save_dir):
    """ç»˜åˆ¶è®­ç»ƒå†å²"""
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='Training Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.grid(True)
    plt.savefig(save_dir / 'training_history.png')
    print(f"âœ… Training history plot saved to {save_dir / 'training_history.png'}")

def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, save_dir):
    """è®­ç»ƒæ¨¡å‹"""
    print("\n=== Starting Training ===")
    best_val_loss = float('inf')
    train_losses = []
    val_losses = []
    
    for epoch in range(EPOCHS):
        print(f"\nEpoch {epoch+1}/{EPOCHS}")
        
        # è®­ç»ƒ
        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, epoch)
        train_losses.append(train_loss)
        
        # éªŒè¯
        val_loss = validate(model, val_loader, criterion)
        val_losses.append(val_loss)
        
        # è°ƒæ•´å­¦ä¹ ç‡
        scheduler.step(val_loss)
        
        # æ‰“å°ç»“æœ
        print(f"Training Loss: {train_loss:.6f}")
        print(f"Validation Loss: {val_loss:.6f}")
        print(f"Current Learning Rate: {optimizer.param_groups[0]['lr']:.6f}")
        
        # ç»˜åˆ¶å½“å‰è®­ç»ƒè¿›åº¦
        if (epoch + 1) % 5 == 0:
            plt.figure(figsize=(10, 6))
            plt.plot(train_losses, label='Training Loss')
            plt.plot(val_losses, label='Validation Loss')
            plt.xlabel('Epoch')
            plt.ylabel('Loss')
            plt.title(f'Training Progress (Epoch {epoch+1}/{EPOCHS})')
            plt.legend()
            plt.grid(True)
            plt.savefig(save_dir / f'training_progress_epoch_{epoch+1}.png')
            plt.close()
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': train_loss,
                'val_loss': val_loss,
                'model_type': 'AttentionMEGDenoiser'
            }, save_dir / 'best_model.pth')
            print(f"âœ… Saved best model (Validation Loss: {val_loss:.6f})")
        
        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % SAVE_EVERY == 0:
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': train_loss,
                'val_loss': val_loss,
                'model_type': 'AttentionMEGDenoiser'
            }, save_dir / f'checkpoint_epoch_{epoch+1}.pth')
            print(f"ğŸ’¾ Saved checkpoint: epoch_{epoch+1}")
        
        # å†…å­˜æ¸…ç†
        gc.collect()
        if train_on_gpu:
            torch.cuda.empty_cache()
    
    print("\nğŸ‰ Training Complete!")
    print(f"Best Validation Loss: {best_val_loss:.6f}")
    print(f"Models saved to: {save_dir}")
    
    # ç»˜åˆ¶å®Œæ•´è®­ç»ƒå†å²
    plot_training_history(train_losses, val_losses, save_dir)
    
    # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œå¯è§†åŒ–
    best_model = AttentionMEGDenoiser(n_channels=1, signal_length=SIGNAL_LENGTH).to(DEVICE)
    checkpoint = torch.load(save_dir / 'best_model.pth')
    best_model.load_state_dict(checkpoint['model_state_dict'])
    
    # ä¿å­˜ä¸€ä¸ªæ ·æœ¬ç»“æœ
    denoised_signal = visualize_denoising_results(best_model, val_loader, save_dir)
    
    result_data = {
        'denoised': denoised_signal,
        'train_losses': train_losses,
        'val_losses': val_losses,
        'best_epoch': checkpoint['epoch'],
        'best_val_loss': best_val_loss
    }
    np.save(save_dir / 'denoising_result.npy', result_data)
    
    # ç»˜åˆ¶æœ€ç»ˆç»“æœ
    plt.figure(figsize=(12, 10))
    plt.subplot(2, 1, 1)
    plt.plot(train_losses, label='Training Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.axvline(x=checkpoint['epoch'], color='r', linestyle='--', label=f'Best Model (Epoch {checkpoint["epoch"]+1})')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training History')
    plt.legend()
    plt.grid(True)
    
    plt.subplot(2, 1, 2)
    for i in range(min(3, denoised_signal.shape[0])):  
        plt.plot(denoised_signal[i], label=f'Channel {i+1}')
    plt.xlabel('Time Points')
    plt.ylabel('Amplitude')
    plt.title('Denoised MEG Signal')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig(save_dir / 'denoising_result.png')
    print(f"âœ… Final result plot saved to {save_dir / 'denoising_result.png'}")
    
    return best_model

def main():
    """ä¸»å‡½æ•°"""
    print("\nğŸš€ Starting Self-Supervised MEG Denoising Model Training")
    
    # åˆ›å»ºç»“æœç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = Path("./output/checkpoints")
    output_dir.mkdir(parents=True, exist_ok=True)
    results_dir = output_dir / f"results_{timestamp}"
    results_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"Results will be saved to: {results_dir}")
    
    # å‡†å¤‡æ•°æ®
    train_loader, val_loader = prepare_data()
    
    # åˆ›å»ºæ¨¡å‹
    print("\n=== Creating Model ===")
    model = AttentionMEGDenoiser(n_channels=1, signal_length=SIGNAL_LENGTH).to(DEVICE)
    print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    # ä½¿ç”¨æ–°çš„è‡ªç›‘ç£æŸå¤±å‡½æ•°
    criterion = SelfSupervisedMEGLoss(gamma=1.0)
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)
    
    # è®­ç»ƒæ¨¡å‹
    train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, results_dir)
    
    print("\nâœ… Training completed!")

if __name__ == "__main__":
    main() 